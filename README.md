<h2 align="center"><em>Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift</em></h2>

<p align="center">
  <strong>
    Jiacheng Cui, Bingkui Tong, Xinyue Bi, Xiaohan Zhao, Jiacheng Liu, Zhiqiang Shen<sup>â€ </sup>
  </strong>
</p>

<p align="center">
Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE &emsp;&emsp;
</p>

<p align="center">
 <sup>â€ </sup>Corresponding Author  
</p>



[![Version](https://img.shields.io/badge/version-1.0.0-blue)]()
[![Arxiv](https://img.shields.io/badge/ðŸ“ƒ-Arxiv-red)](TODO)
[![Open In Spaces](https://img.shields.io/badge/GoogleDrive-Dataset-blue?logo=google-drive&logoColor=white)](https://drive.google.com/drive/folders/1wLm_jZA8bt95Tv8DDYyftheOnDdGgRnW?usp=drive_link)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)]()
[![GitHub Repo stars](https://img.shields.io/github/stars/Jiacheng8/HALD)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Contact](https://img.shields.io/badge/Contact-Jicheng%20Cui-green)](mailto:Jiacheng.Cui@mbzuai.ac.ae)

## ðŸ“° News

- ðŸš€ **[2025-]** Weâ€™re thrilled to release **HALD v1.0.0**. Everything is ready for plug-and-play reproducibility and benchmarking. ðŸ‘‰ [Grab it here](https://drive.google.com/drive/folders/1wLm_jZA8bt95Tv8DDYyftheOnDdGgRnW?usp=drive_link)



## ðŸš€ Overview
Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 41.8% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 8.1%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-labelâ€“dominated training.

<p align="center">
<img src="./img/intro_fig.png" width=50% height=50% class="center">
</p>



### â­ Contributions
We revisit the role of hard labels in **dataset distillation** and introduce a hybrid training paradigm, **H**ard Label for **A**lleviating **L**ocal Semantic **D**rift (**HALD**).  
Our core idea is to *strategically combine hard and soft labels*:  

- Hard labels recalibrate the semantic space of image crops  
- Soft labels preserve fine-grained, nuanced supervision  

From a theoretical perspective, we show that using only a limited number of soft labels inevitably induces **local semantic drift**. We then mathematically demonstrate how integrating hard labels effectively counteracts this drift. Extensive experiments across multiple benchmarks validate that **HALD**:

âœ… Reduces distribution mismatch  
âœ… Improves generalization  
âœ… Remains robust even under **aggressive soft-label compression**


## ðŸ› ï¸ Installation Guide

Follow these steps to set up the environment and access necessary resources:



### ðŸ“¦ Step 1: Clone the Repository
```bash
git https://github.com/Jiacheng8/HALD.git
cd HALD
```



### ðŸ§ª Step 2: Set Up Conda Environment
Create and activate the PyTorch environment using the provided configuration:
```bash
conda env create -f environment.yml
conda activate HALD
```

### ðŸ“ Step 3: Download Required Files
Some files must be downloaded manually from Google Drive. After downloading, update the corresponding paths in the `config.sh` file.

- ðŸ”— [Download Distilled Images](https://drive.google.com/drive/folders/1H99ui_LZxyNYfaAEfDVX7rvobP3J-Lwp?usp=drive_link)
- ðŸ”— [Download Validation Images](https://drive.google.com/drive/folders/1BCWWdvo1MnwtTUIEK-t3QcS0KwwAMVqH?usp=drive_link)

> ðŸ“Œ **Reminder:** Donâ€™t forget to update `config.sh`!  
> Set `Main_Data_Path` to the folder where your generated data is saved, this is crucial for proper loading. Please put the pretrained models under the `Main_Data_Path`. There is no restriction for where you should put the patches or the validation set, as long as you update them correctly in `config.sh`.

## ðŸš€ Running Experiments

All experiment pipelines are modularized into **three stages**:

2. ðŸ·ï¸ `relabel/` â€” Generate the corresponding **soft labels**  
3. ðŸ“Š `validate/` â€” **Evaluate** the performance of the distilled dataset


## ðŸ“š Bibliography

If you find this repository helpful for your research or project, please consider citing our work:

> ðŸ”– **Citation (BibTeX):**
```bibtex

```

ðŸ“Œ *Your citation helps support and acknowledge our research contributions to the dataset distillation community.*